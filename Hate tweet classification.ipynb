{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "Import both train and test files and separate the label (target variable) from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "original_sentences= train['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data\n",
    "Since the data is basically tweets, it contains various symbols and numbers. During, the preprocessing of data, anything other than numbers and hashtags and removed, stemmed and tf-idf is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data,pattern):\n",
    "    r = re.findall(pattern,data)\n",
    "    for i in r:\n",
    "        data = re.sub(i,\"\",data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = np.vectorize(clean)(train['tweet'],\"@[\\w]*\")\n",
    "train['tweet'] = train['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "tokenised = train['tweet'].apply(lambda x: x.split())\n",
    "#print(type(tokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "tokenised = tokenised.apply(lambda x: [ps.stem(i) for i in x])\n",
    "for i in range(len(tokenised)):\n",
    "    tokenised[i] = \" \".join(tokenised[i])\n",
    "train['tweet'] = tokenised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              tweet\n",
      "0  31963  #studiolif #aislif #requir #passion #dedic #wi...\n",
      "1  31964  #white #supremacist want everyon to see the ne...\n",
      "2  31965  safe way to heal your #acn #altwaystoh #health...\n",
      "3  31966  is the hp and the curs child book up for reser...\n",
      "4  31967  rd #bihday to my amaz hilari #nephew eli ahmir...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>when a father is dysfunct and is so selfish he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>thank for #lyft credit i can t use caus they d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>bihday your majesti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#model i love u take with u all the time in ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>factsguid societi now #motiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>huge fan fare and big talk befor they leav cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>camp tomorrow danni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>the next school year is the year for exam can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>we won love the land #allin #cav #champion #cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>welcom here i m it s so #gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>#ireland consum price index mom climb from pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>we are so selfish #orlando #standwithorlando #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>i get to see my daddi today # day #gettingf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>#cnn call #michigan middl school build the wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>no comment in #australia #opkillingbay #seashe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>ouch junior is angri #got #junior #yugyoem #omg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>i am thank for have a paner #thank #posit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>retweet if you agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>it #friday smile all around via ig user #cooki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>as we all know essenti oil are not made of chemic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>#euro peopl blame ha for conced goal wa it fat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>sad littl dude #badday #coneofsham #cat #piss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>product of the day happi man #wine tool who s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>lumpi say i am a prove it lumpi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>#tgif #ff to my #gamedev #indiedev #indiegamed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>beauti sign by vendor for #upsideofflorida #sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>all #smile when #media is #pressconfer in #ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>we had a great panel on the mediat of the publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>happi father s day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>peopl went to nightclub to have a good night a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31932</th>\n",
       "      <td>31933</td>\n",
       "      <td>thank gemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31933</th>\n",
       "      <td>31934</td>\n",
       "      <td>judd is a amp #homophob #freemilo #milo #freem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31934</th>\n",
       "      <td>31935</td>\n",
       "      <td>ladi ban from kentucki mall #jcpenni #kentucki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31935</th>\n",
       "      <td>31936</td>\n",
       "      <td>ugh i m tri to enjoy my happi hour drink amp t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31936</th>\n",
       "      <td>31937</td>\n",
       "      <td>want to know how to live a life do more thing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31937</th>\n",
       "      <td>31938</td>\n",
       "      <td>love island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31938</th>\n",
       "      <td>31939</td>\n",
       "      <td>my fav actor #vijaysethupathi my fav actress m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31939</th>\n",
       "      <td>31940</td>\n",
       "      <td>whew it s a product and #friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31940</th>\n",
       "      <td>31941</td>\n",
       "      <td>she s final here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31941</th>\n",
       "      <td>31942</td>\n",
       "      <td>pass first year of uni #yay #love #pass #unist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31942</th>\n",
       "      <td>31943</td>\n",
       "      <td>thi week is fli by #humpday #wednesday #kamp #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31943</th>\n",
       "      <td>31944</td>\n",
       "      <td>model photoshoot thi friday yay #model #me #fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31944</th>\n",
       "      <td>31945</td>\n",
       "      <td>you re surround by peopl who love you even mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31945</th>\n",
       "      <td>31946</td>\n",
       "      <td>feel like #dog #summer #hot #help #sun #day #more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31946</th>\n",
       "      <td>31947</td>\n",
       "      <td>omfg i m offend i m a mailbox and i m proud #m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31947</th>\n",
       "      <td>31948</td>\n",
       "      <td>you don t have the ball to hashtag me as a but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31948</th>\n",
       "      <td>31949</td>\n",
       "      <td>make you ask yourself who am i then am i anybo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31949</th>\n",
       "      <td>31950</td>\n",
       "      <td>hear one of my new song don t go kati elli #yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31950</th>\n",
       "      <td>31951</td>\n",
       "      <td>you can tri to tail us to stop butt we re just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31951</th>\n",
       "      <td>31952</td>\n",
       "      <td>i ve just post a new blog #secondlif #lone #neko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31952</th>\n",
       "      <td>31953</td>\n",
       "      <td>you went too far with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31953</th>\n",
       "      <td>31954</td>\n",
       "      <td>good morn #instagram #shower #water #berlin #b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31954</th>\n",
       "      <td>31955</td>\n",
       "      <td>#holiday bull up you will domin your bull and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31955</th>\n",
       "      <td>31956</td>\n",
       "      <td>less than week #ibiza#bringiton#mallorca#holid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31956</th>\n",
       "      <td>31957</td>\n",
       "      <td>off fish tomorrow carnt wait first time in year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>ate isz that youuu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>to see nina turner on the airwav tri to wrap h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>listen to sad song on a monday morn otw to wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>#sikh #templ vandalis in in #calgari #wso cond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>thank you for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet\n",
       "0          1  when a father is dysfunct and is so selfish he...\n",
       "1          2  thank for #lyft credit i can t use caus they d...\n",
       "2          3                                bihday your majesti\n",
       "3          4     #model i love u take with u all the time in ur\n",
       "4          5                       factsguid societi now #motiv\n",
       "5          6  huge fan fare and big talk befor they leav cha...\n",
       "6          7                                camp tomorrow danni\n",
       "7          8  the next school year is the year for exam can ...\n",
       "8          9  we won love the land #allin #cav #champion #cl...\n",
       "9         10                        welcom here i m it s so #gr\n",
       "10        11  #ireland consum price index mom climb from pre...\n",
       "11        12  we are so selfish #orlando #standwithorlando #...\n",
       "12        13        i get to see my daddi today # day #gettingf\n",
       "13        14  #cnn call #michigan middl school build the wal...\n",
       "14        15  no comment in #australia #opkillingbay #seashe...\n",
       "15        16    ouch junior is angri #got #junior #yugyoem #omg\n",
       "16        17          i am thank for have a paner #thank #posit\n",
       "17        18                                retweet if you agre\n",
       "18        19  it #friday smile all around via ig user #cooki...\n",
       "19        20  as we all know essenti oil are not made of chemic\n",
       "20        21  #euro peopl blame ha for conced goal wa it fat...\n",
       "21        22  sad littl dude #badday #coneofsham #cat #piss ...\n",
       "22        23  product of the day happi man #wine tool who s ...\n",
       "23        24                    lumpi say i am a prove it lumpi\n",
       "24        25  #tgif #ff to my #gamedev #indiedev #indiegamed...\n",
       "25        26  beauti sign by vendor for #upsideofflorida #sh...\n",
       "26        27  all #smile when #media is #pressconfer in #ant...\n",
       "27        28  we had a great panel on the mediat of the publ...\n",
       "28        29                                 happi father s day\n",
       "29        30  peopl went to nightclub to have a good night a...\n",
       "...      ...                                                ...\n",
       "31932  31933                                        thank gemma\n",
       "31933  31934  judd is a amp #homophob #freemilo #milo #freem...\n",
       "31934  31935     ladi ban from kentucki mall #jcpenni #kentucki\n",
       "31935  31936  ugh i m tri to enjoy my happi hour drink amp t...\n",
       "31936  31937  want to know how to live a life do more thing ...\n",
       "31937  31938                                        love island\n",
       "31938  31939  my fav actor #vijaysethupathi my fav actress m...\n",
       "31939  31940                    whew it s a product and #friday\n",
       "31940  31941                                   she s final here\n",
       "31941  31942  pass first year of uni #yay #love #pass #unist...\n",
       "31942  31943  thi week is fli by #humpday #wednesday #kamp #...\n",
       "31943  31944  model photoshoot thi friday yay #model #me #fo...\n",
       "31944  31945  you re surround by peopl who love you even mor...\n",
       "31945  31946  feel like #dog #summer #hot #help #sun #day #more\n",
       "31946  31947  omfg i m offend i m a mailbox and i m proud #m...\n",
       "31947  31948  you don t have the ball to hashtag me as a but...\n",
       "31948  31949  make you ask yourself who am i then am i anybo...\n",
       "31949  31950  hear one of my new song don t go kati elli #yo...\n",
       "31950  31951  you can tri to tail us to stop butt we re just...\n",
       "31951  31952   i ve just post a new blog #secondlif #lone #neko\n",
       "31952  31953                              you went too far with\n",
       "31953  31954  good morn #instagram #shower #water #berlin #b...\n",
       "31954  31955  #holiday bull up you will domin your bull and ...\n",
       "31955  31956  less than week #ibiza#bringiton#mallorca#holid...\n",
       "31956  31957    off fish tomorrow carnt wait first time in year\n",
       "31957  31958                                 ate isz that youuu\n",
       "31958  31959  to see nina turner on the airwav tri to wrap h...\n",
       "31959  31960  listen to sad song on a monday morn otw to wor...\n",
       "31960  31961  #sikh #templ vandalis in in #calgari #wso cond...\n",
       "31961  31962                           thank you for you follow\n",
       "\n",
       "[31962 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test_sentences = test['tweet']\n",
    "test['tweet'] = np.vectorize(clean)(test['tweet'],\"@[\\w]*\")\n",
    "test['tweet'] = test['tweet'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "trial = test['tweet'].apply(lambda x: x.split())\n",
    "trial = trial.apply(lambda x: [ps.stem(i) for i in x])\n",
    "for i in range(len(trial)):\n",
    "    trial[i] = \" \".join(trial[i])\n",
    "test['tweet'] = trial\n",
    "print(test[:5])\n",
    "label = train['label']\n",
    "train.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 301)\t0.33582193077220196\n",
      "  (0, 407)\t0.726320198436431\n",
      "  (0, 464)\t0.4119910421650475\n",
      "  (0, 739)\t0.43582792628679934\n",
      "  (1, 867)\t0.32744560137262047\n",
      "  (1, 919)\t0.42095781882008054\n",
      "  (1, 139)\t0.5109807291435251\n",
      "  (1, 243)\t0.36819619826595434\n",
      "  (1, 602)\t0.5647159878579786\n",
      "  (2, 87)\t1.0\n",
      "  (3, 560)\t0.5804402378096797\n",
      "  (3, 517)\t0.3617512633323277\n",
      "  (3, 877)\t0.45833214408664846\n",
      "  (3, 916)\t0.5675885829593854\n",
      "  (4, 569)\t1.0\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tfidf = TfidfVectorizer(max_df = 0.9,  min_df = 2, max_features = 1000,stop_words = 'english')\n",
    "#train_tfidf = tfidf.fit_transform(train['tweet'])\n",
    "#print(train_tfidf[:5])\n",
    "#test_tfidf = tfidf.fit_transform(test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow_train = bow_vectorizer.fit_transform(train['tweet'])\n",
    "bow_test = bow_vectorizer.fit_transform(test['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "seed = 30\n",
    "test_size = 0.3\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(bow_train,label,test_size = test_size, random_state = seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.46156103286386\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,Y_train)\n",
    "valid = KFold(n_splits = 15, random_state = 10)\n",
    "results = cross_val_score(model,X_test,Y_test,cv=valid)\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "#accuracy = accuracy_score(y_pred,Y_test)\n",
    "print(results.mean()*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  label\n",
      "0  31963      0\n",
      "1  31964      0\n",
      "2  31965      0\n",
      "3  31966      0\n",
      "4  31967      0\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(bow_test)\n",
    "test['label'] = prediction\n",
    "submission = test[['id','label']]\n",
    "submission.to_csv('sub_rf2_bow.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
